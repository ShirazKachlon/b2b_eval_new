{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path(os.getcwd()).parent.parent.parent)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/tamir/workspace/automotive/src/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanes Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _has_road_boundaries(elem):\n",
    "    if 'road_boundary_right' in elem and 'road_boundary_left' in elem:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def plot_set_statistics(eval_output_path, set_name):\n",
    "    \n",
    "    # load lanes json\n",
    "    lanes_json_path = eval_output_path.joinpath('inputs/lanes.json')\n",
    "    lanes = json.load(open(lanes_json_path))\n",
    "\n",
    "    # load gt dataframe\n",
    "    df = pd.read_csv(eval_output_path.joinpath('inputs/b2b_det_complete.tsv'), sep='\\t')\n",
    "    df['id'] = df.name.apply(lambda x: x.split('.')[0])\n",
    "\n",
    "    total_frames = len(df.name.unique())\n",
    "    total_lanes_found_per_frame = [len(lanes['frames'][key].keys()) if key in lanes['frames'] else 0 for key in df.id.unique()]\n",
    "    has_both_boundaries = [_has_road_boundaries(lanes['frames'][elem]) for elem in lanes['frames'].keys()]\n",
    "    \n",
    "    bar_values = np.bincount(has_both_boundaries)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle(set_name)\n",
    "\n",
    "    bars = axs[0].bar(range(len(bar_values)), bar_values, align='center')\n",
    "    # Adding labels and title\n",
    "    axs[0].set_xlabel('Has both boundaries')\n",
    "    axs[0].set_ylabel('Counts')\n",
    "    axs[0].set_title('Total has both boundaries')\n",
    "\n",
    "    # Adding values on top of the bars\n",
    "    for bar, count in zip(bars, bar_values):\n",
    "        axs[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    # Displaying the plot\n",
    "\n",
    "    bar_values = np.bincount(total_lanes_found_per_frame)\n",
    "    bars = axs[1].bar(range(len(bar_values)), bar_values, align='center')\n",
    "\n",
    "    # Adding labels and title\n",
    "    axs[1].set_xlabel('Total found')\n",
    "    axs[1].set_ylabel('Counts')\n",
    "    axs[1].set_title('Total lanes found per frame')\n",
    "\n",
    "    # Adding values on top of the bars\n",
    "    for bar, count in zip(bars, bar_values):\n",
    "        axs[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height(), str(count),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    # Displaying the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_names = [\n",
    "    '64f45788ac243656c2bd4986',\n",
    "    '64b3729f628d0605eb5a082d',\n",
    "    '64b63235628d0605eb5a1722',\n",
    "    '64f561c6ac243656c2bd4fcd',\n",
    "    '64f56176ac243656c2bd4fbb',\n",
    "    '64f56183ac243656c2bd4fbe',\n",
    "    \n",
    "]\n",
    "\n",
    "for set_name in set_names:\n",
    "    eval_output_path = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval')\n",
    "    plot_set_statistics(eval_output_path, set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set, visualizing the affect of lane filtering on precision & recall for different classes.\n",
    "Main logic:\n",
    "* Put any GT detection that is out of boundary as lanes ignore.\n",
    "* Any prediction (prod) detection has grace margin to tackle cases where the detection is marked as ignore and gt is note (cause unwanted behaviour by the evaluation script)\n",
    "* Peds has different logic than veichles, they has higher margin from boundary so only very far pedestrains are ignored. This is important duo to the fact that this filter filter almost all pedestrians.\n",
    "* When no lanes are detected or when not both boundaries are present, the entire image is filtered.\n",
    "* * When there is no cametra data it considered as no lanes.\n",
    "* * When a lane has too few points inside an image (for example less than 20 out of 50) it is considered as None. Happens when a line is very horizontal instead of vertical.\n",
    "* For GT detections, any object that is not in EGO \\ NEXT RIGHT \\ NEXT LEFT lane is ignored. Works only if the relevent lanes are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall(x):\n",
    "    a = x[1:].split('/')\n",
    "    if int(a[1]) == 0:\n",
    "        return 0\n",
    "    return int(a[0]) / int(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(dfs: List[pd.DataFrame], class_name: str, title='Compare'):\n",
    "    dfs = [df[df['class_name'] == class_name] for df in dfs]\n",
    "    \n",
    "    # Set up the subplot\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 1 + 3 * len(dfs)))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.2\n",
    "    total_bars_width = (bar_width + 0.1) * len(dfs) + 0.2\n",
    "    initial_positions = np.arange(len(dfs[0])) * total_bars_width\n",
    "    \n",
    "    columns = ['precision_loose', 'evaluated_recall', 'samples']\n",
    "    \n",
    "    # Set positions for the bars\n",
    "    for i, df in enumerate(dfs):\n",
    "        positions = initial_positions + i * (bar_width + 0.1)\n",
    "        for k, col in enumerate(columns):\n",
    "            axs[k].barh(positions, df[col], height=bar_width, label=df.name.values[0])\n",
    "            for j, value in enumerate(df[col]):\n",
    "                axs[k].text(value, positions[j], f'{value:.2f}', ha='right', va='center')\n",
    "        \n",
    "        # axs[0].barh(positions, df['precision_loose'], height=bar_width, label=df.name.values[0])\n",
    "        # axs[1].barh(positions, df['evaluated_recall'], height=bar_width, label=df.name.values[0])\n",
    "        # axs[2].barh(positions, df['samples'], height=bar_width, label=df.name.values[0])\n",
    "\n",
    "        \n",
    "            \n",
    "        # for j, value in enumerate(df['evaluated_recall']):\n",
    "        #     axs[1].text(value, positions[j], f'{value:.2f}', ha='right', va='center')\n",
    "\n",
    "    # Set y-axis ticks and labels\n",
    "    axs[0].set_yticks(initial_positions, dfs[0]['bin'])\n",
    "    axs[0].set_xlabel('Precision')\n",
    "    axs[0].set_ylabel('Bins')\n",
    "    axs[0].set_title(f'Precision: {class_name}')\n",
    "    axs[0].legend()  # Display legend\n",
    "    \n",
    "    # Set y-axis ticks and labels\n",
    "    axs[1].set_yticks(initial_positions, dfs[0]['bin'])\n",
    "    axs[1].set_xlabel('Recall')\n",
    "    axs[1].set_ylabel('Bins')\n",
    "    axs[1].set_title(f'Recall: {class_name}')\n",
    "    axs[1].legend()  # Display legend\n",
    "\n",
    "    # Set y-axis ticks and labels\n",
    "    axs[2].set_yticks(initial_positions, dfs[0]['bin'])\n",
    "    axs[2].set_xlabel('Samples')\n",
    "    axs[2].set_ylabel('Bins')\n",
    "    axs[2].set_title(f'Samples: {class_name}')\n",
    "    axs[2].legend()  # Display legend\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f56183ac243656c2bd4fbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_baseline = Path('/home/tamir/workspace/AB_B2B_Eval/tmp/64f56183ac243656c2bd4fbe_eval_baseline')\n",
    "df_baseline = pd.read_csv(eval_baseline.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_baseline['name'] = 'baseline'\n",
    "df_baseline['evaluated_recall'] = df_baseline.recall.apply(calc_recall)\n",
    "baselines['64f56183ac243656c2bd4fbe'] = df_baseline\n",
    "\n",
    "plot_comparison(dfs=[baselines['64f56183ac243656c2bd4fbe']], class_name='4w', title='64f56183ac243656c2bd4fbe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f561c6ac243656c2bd4fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_baseline = Path('/home/tamir/workspace/AB_B2B_Eval/tmp/64f561c6ac243656c2bd4fcd_eval_baseline')\n",
    "df_baseline = pd.read_csv(eval_baseline.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_baseline['name'] = 'baseline'\n",
    "df_baseline['evaluated_recall'] = df_baseline.recall.apply(calc_recall)\n",
    "baselines['64f561c6ac243656c2bd4fcd'] = df_baseline\n",
    "\n",
    "plot_comparison(dfs=[baselines['64f561c6ac243656c2bd4fcd']], class_name='4w', title='64f561c6ac243656c2bd4fcd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f45788ac243656c2bd4986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f45788ac243656c2bd4986'\n",
    "eval_baseline = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval_baseline')\n",
    "df_baseline = pd.read_csv(eval_baseline.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_baseline['name'] = 'baseline'\n",
    "df_baseline['evaluated_recall'] = df_baseline.recall.apply(calc_recall)\n",
    "baselines[set_name] = df_baseline\n",
    "\n",
    "plot_comparison(dfs=[baselines[set_name]], class_name='4w', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval with Lanes (v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_lanes_v1 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f56183ac243656c2bd4fbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f56183ac243656c2bd4fbe'\n",
    "\n",
    "eval_with_lanes_v1 = Path(f'/home/tamir/s3_sync/europe_run_3_10/{set_name}/eval_on_infer_0.1.4/prod_sf_vs_al')\n",
    "df_with_lanes_v1 = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_with_lanes_v1['name'] = 'with_lanes'\n",
    "df_with_lanes_v1['evaluated_recall'] = df_with_lanes_v1.recall.apply(calc_recall)\n",
    "results_with_lanes_v1[set_name] = df_with_lanes_v1\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f561c6ac243656c2bd4fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f561c6ac243656c2bd4fcd'\n",
    "\n",
    "eval_with_lanes_v1 = Path(f'/home/tamir/s3_sync/europe_run_3_10/{set_name}/eval_on_infer_0.1.4/prod_sf_vs_al')\n",
    "df_with_lanes_v1 = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_with_lanes_v1['name'] = 'with_lanes'\n",
    "df_with_lanes_v1['evaluated_recall'] = df_with_lanes_v1.recall.apply(calc_recall)\n",
    "results_with_lanes_v1[set_name] = df_with_lanes_v1\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f45788ac243656c2bd4986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f45788ac243656c2bd4986'\n",
    "\n",
    "eval_with_lanes_v1 = Path(f'/home/tamir/s3_sync/europe_run_3_10/{set_name}/eval_on_infer_0.1.4/prod_sf_vs_al')\n",
    "df_with_lanes_v1 = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_with_lanes_v1['name'] = 'with_lanes'\n",
    "df_with_lanes_v1['evaluated_recall'] = df_with_lanes_v1.recall.apply(calc_recall)\n",
    "results_with_lanes_v1[set_name] = df_with_lanes_v1\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval with lanes (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_lanes_v2 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f56183ac243656c2bd4fbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f56183ac243656c2bd4fbe'\n",
    "\n",
    "eval_with_lanes_v2 = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval_v2')\n",
    "df_with_lanes_v2 = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_with_lanes_v2['name'] = 'with_lanes'\n",
    "df_with_lanes_v2['evaluated_recall'] = df_with_lanes_v2.recall.apply(calc_recall)\n",
    "results_with_lanes_v2[set_name] = df_with_lanes_v2\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name], results_with_lanes_v2[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f561c6ac243656c2bd4fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f561c6ac243656c2bd4fcd'\n",
    "\n",
    "eval_with_lanes_v2 = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval_v2')\n",
    "df_with_lanes_v2 = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_with_lanes_v2['name'] = 'with_lanes_v2'\n",
    "df_with_lanes_v2['evaluated_recall'] = df_with_lanes_v2.recall.apply(calc_recall)\n",
    "results_with_lanes_v2[set_name] = df_with_lanes_v2\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name], results_with_lanes_v2[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f45788ac243656c2bd4986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f45788ac243656c2bd4986'\n",
    "\n",
    "eval_with_lanes_v2 = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval_v2')\n",
    "df_with_lanes_v2 = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df_with_lanes_v2['name'] = 'with_lanes_v2'\n",
    "df_with_lanes_v2['evaluated_recall'] = df_with_lanes_v2.recall.apply(calc_recall)\n",
    "results_with_lanes_v2[set_name] = df_with_lanes_v2\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name], results_with_lanes_v2[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the next pivot example, there are cases where there is a detection by prod that is out of boundaries.\n",
    "This detection does not have an ignore GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval with lanes (v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing grace gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_lanes_v3 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f561c6ac243656c2bd4fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f561c6ac243656c2bd4fcd'\n",
    "\n",
    "eval = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval_v3')\n",
    "df = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df['name'] = 'with_lanes_v3'\n",
    "df['evaluated_recall'] = df.recall.apply(calc_recall)\n",
    "results_with_lanes_v3[set_name] = df\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name], results_with_lanes_v2[set_name], results_with_lanes_v3[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 64f56183ac243656c2bd4fbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f56183ac243656c2bd4fbe'\n",
    "\n",
    "eval = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval_v3')\n",
    "df = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df['name'] = 'with_lanes_v3'\n",
    "df['evaluated_recall'] = df.recall.apply(calc_recall)\n",
    "results_with_lanes_v3[set_name] = df\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name], results_with_lanes_v2[set_name], results_with_lanes_v3[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 64f45788ac243656c2bd4986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f45788ac243656c2bd4986'\n",
    "\n",
    "eval = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/{set_name}_eval_v3')\n",
    "df = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df['name'] = 'with_lanes_v3'\n",
    "df['evaluated_recall'] = df.recall.apply(calc_recall)\n",
    "results_with_lanes_v3[set_name] = df\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name], results_with_lanes_v2[set_name], results_with_lanes_v3[set_name]]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = '64f45788ac243656c2bd4986'\n",
    "\n",
    "eval = Path(f'/home/tamir/workspace/AB_B2B_Eval/tmp/64f45788ac243656c2bd4986_eval_test')\n",
    "df = pd.read_csv(eval_with_lanes_v1.joinpath('summary/eval_table_results_max_recall_test.tsv'), sep='\\t')\n",
    "df['name'] = 'with_lanes_test'\n",
    "df['evaluated_recall'] = df.recall.apply(calc_recall)\n",
    "results_with_lanes_v3['test'] = df\n",
    "\n",
    "dfs = [baselines[set_name], results_with_lanes_v1[set_name], results_with_lanes_v2[set_name], results_with_lanes_v3['test']]\n",
    "plot_comparison(dfs=dfs, class_name='4w', title=set_name)\n",
    "plot_comparison(dfs=dfs, class_name='peds', title=set_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
